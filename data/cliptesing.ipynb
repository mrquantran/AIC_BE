{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-31T08:15:31.815739Z","iopub.status.busy":"2024-07-31T08:15:31.814819Z","iopub.status.idle":"2024-07-31T08:18:42.835440Z","shell.execute_reply":"2024-07-31T08:18:42.834316Z","shell.execute_reply.started":"2024-07-31T08:15:31.815703Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip -q install open_clip_torch\n","%pip -q install transformers\n","%pip -q install torch\n","%pip -q install pillow\n","%pip -q install numpy\n","%pip -q install torch\n","%pip -q install tqdm\n","%pip -q install opencv-python\n","%pip -q install imagehash\n","%pip -q install ffmpeg-python\n","%pip -q install einops\n","%pip -q install faiss-cpu\n","%pip -q install usearch\n","%pip -q install translate\n","%pip -q install googletrans\n","%pip -q install pillow\n","%pip -q install matplotlib"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: httpcore in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (from httpcore) (0.9.0)\n","Requirement already satisfied: h2==3.* in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (from httpcore) (3.2.0)\n","Requirement already satisfied: sniffio==1.* in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (from httpcore) (1.3.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (from h2==3.*->httpcore) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages (from h2==3.*->httpcore) (3.0.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install httpcore"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:18:42.837851Z","iopub.status.busy":"2024-07-31T08:18:42.837521Z","iopub.status.idle":"2024-07-31T08:18:46.246051Z","shell.execute_reply":"2024-07-31T08:18:46.245307Z","shell.execute_reply.started":"2024-07-31T08:18:42.837821Z"},"trusted":true},"outputs":[],"source":["# standard lib\n","import os\n","import re\n","import json\n","import io\n","import base64\n","from typing import List, Dict, Any, Tuple\n","\n","# numerical computing\n","import numpy as np\n","\n","# Deep Learning and AI:\n","import torch\n","import open_clip\n","\n","#img processing\n","from PIL import Image\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","\n","# Progress Tracking\n","from tqdm.notebook import tqdm\n","\n","#indexing and searchin:\n","import faiss\n","from usearch.index import Index as UsearchIndex\n","\n","# translation\n","import googletrans\n","import translate\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:30:16.747984Z","iopub.status.busy":"2024-07-31T08:30:16.747597Z","iopub.status.idle":"2024-07-31T08:30:16.783473Z","shell.execute_reply":"2024-07-31T08:30:16.782328Z","shell.execute_reply.started":"2024-07-31T08:30:16.747956Z"},"trusted":true},"outputs":[],"source":["class CLIPEmbedding:\n","    def __init__(\n","        self, \n","        model_name: str,\n","        model_nick_name: str,\n","        device: str = None\n","    ):\n","        self.model_nick_name = model_nick_name\n","        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        try:\n","            print(f\"Attempting to load model on {self.device}\")\n","            self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name)\n","            self.model = self.model.to(self.device)\n","        except RuntimeError as e:\n","            if 'out of memory' in str(e):\n","                print(\"GPU out of memory. Falling back to CPU.\")\n","                self.device = 'cpu'\n","                self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name)\n","                self.model = self.model.to(self.device)\n","            else:\n","                raise e\n","\n","        self.model.eval()\n","        self.tokenizer = open_clip.get_tokenizer(model_name)\n","        self.faiss_index = None\n","        self.usearch_index = None\n","        self.global_index2img_path = {}\n","    \n","    def process_image_folder(\n","        self,\n","        root_dir: str,\n","        output_dir: str,\n","        batch_size: int = 32\n","    ):\n","        os.makedirs(output_dir, exist_ok=True)\n","        \n","        image_paths = []\n","        for root, _, files in os.walk(root_dir):\n","            for file in files:\n","                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp')):\n","                    image_paths.append(os.path.join(root, file))\n","        \n","        if not image_paths:\n","            print(f\"No image found in the given root directory: {root_dir}\")\n","            return None\n","\n","        image_paths.sort()\n","        \n","        embeddings = []\n","        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing Batches of images\", unit=f'batch, size = {batch_size}'):\n","            batch_paths = image_paths[i:i+batch_size]\n","            batch_embeddings = self.process_batch(batch_paths)\n","            if batch_embeddings is not None:\n","                embeddings.append(batch_embeddings)\n","        \n","        if embeddings:\n","            all_embeddings = np.vstack(embeddings)\n","            \n","            # Save CLIP embeddings\n","            clip_file = os.path.join(output_dir, f'{self.model_nick_name}_clip_embeddings.npy')\n","            np.save(clip_file, all_embeddings)\n","            print(f\"CLIP embeddings saved to {clip_file}\")\n","            \n","            # Save global_index2img_path\n","            self.global_index2img_path = {i: path for i, path in enumerate(image_paths)}\n","            index_path_file = os.path.join(output_dir, 'global2imgpath.json')\n","            with open(index_path_file, 'w') as f:\n","                json.dump(self.global_index2img_path, f, indent=4)\n","            print(f\"global2imgpath saved to {index_path_file}\")\n","            \n","            # Build and save FAISS index\n","            self.build_faiss_index(all_embeddings)\n","            faiss_file = os.path.join(output_dir, f\"{self.model_nick_name}_faiss.bin\")\n","            self.save_faiss_index(faiss_file)\n","            \n","            # Build and save USearch index\n","            self.build_usearch_index(all_embeddings)\n","            usearch_file = os.path.join(output_dir, f\"{self.model_nick_name}_usearch.bin\")\n","            self.save_usearch_index(usearch_file)\n","            \n","            return all_embeddings\n","        else:\n","            print(\"No embeddings were created.\")\n","            return None\n","\n","    def process_batch(self, batch_paths):\n","        batch_images = []\n","        for img_path in batch_paths:\n","            try:\n","                img = Image.open(img_path).convert('RGB')\n","                img_tensor = self.preprocess(img).unsqueeze(0)\n","                batch_images.append(img_tensor)\n","            except Exception as e:\n","                print(f\"Error processing image {img_path}: {str(e)}\")\n","                continue\n","        \n","        if batch_images:\n","            batch_tensor = torch.cat(batch_images).to(self.device)\n","            with torch.no_grad():\n","                batch_embeddings = self.model.encode_image(batch_tensor).cpu().detach().numpy().astype(np.float32)\n","            return batch_embeddings\n","        return None\n","    \n","    def build_faiss_index(self, embeddings: np.ndarray):\n","        dimension = embeddings.shape[1]\n","        self.faiss_index = faiss.IndexFlatIP(dimension)\n","        self.faiss_index.add(embeddings)\n","    \n","    def save_faiss_index(self, file_path: str):\n","        faiss.write_index(self.faiss_index, file_path)\n","        print(f\"FAISS index saved to {file_path}\")\n","\n","    def load_faiss_index(self, file_path: str):\n","        self.faiss_index = faiss.read_index(file_path)\n","        print(f\"FAISS index loaded from {file_path}\")\n","        \n","        \n","    def build_usearch_index(self, embeddings: np.ndarray):\n","        dimension = embeddings.shape[1]\n","        self.usearch_index = UsearchIndex(ndim=dimension, metric='cosine')\n","        for i, embedding in enumerate(embeddings):\n","            self.usearch_index.add(i, embedding)\n","\n","    def save_usearch_index(self, file_path: str):\n","        self.usearch_index.save(file_path)\n","        print(f\"USearch index saved to {file_path}\")\n","\n","    def load_usearch_index(self, file_path: str):\n","        dimension = self.faiss_index.d\n","        self.usearch_index = UsearchIndex(ndim=dimension, metric='cosine')\n","        self.usearch_index.load(file_path)\n","        print(f\"USearch index loaded from {file_path}\")\n","    \n","    def faiss_search(self, query_embedding: np.ndarray, k: int) -> List[Tuple[int, float]]:\n","        faiss.normalize_L2(query_embedding)\n","        distances, indices = self.faiss_index.search(query_embedding, k)\n","        return list(zip(indices[0], distances[0]))\n","    def usearch_search(self, query_embedding: np.ndarray, k: int) -> List[Tuple[int, float]]:\n","        matches = self.usearch_index.search(query_embedding, k)\n","        return [(int(match.key), match.distance) for match in matches]\n","        \n","        \n","    def text_query(self, query: str, k: int = 20) -> Tuple[List[Tuple[int, float]], List[Tuple[int, float]]]:\n","        with torch.no_grad():\n","            text_tokens = self.tokenizer([query]).to(self.device)\n","            query_embedding = self.model.encode_text(text_tokens).cpu().detach().numpy().astype(np.float32)\n","        \n","        faiss_results = self.faiss_search(query_embedding, k)\n","        usearch_results = self.usearch_search(query_embedding[0], k)\n","        \n","        return faiss_results, usearch_results\n","    \n","    def image_query(self, img_data: str, k: int = 20) -> Tuple[List[Tuple[int, float]], List[Tuple[int, float]]]:\n","        \n","        img_bytes = base64.b64decode(img_data)\n","        img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n","        \n","        img_preprocessed = self.preprocess(img).unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            query_embedding = self.model.encode_image(img_preprocessed).cpu().detach().numpy().astype(np.float32)\n","        \n","        faiss_results = self._faiss_search(query_embedding, k)\n","        usearch_results = self._usearch_search(query_embedding[0], k)\n","        \n","        return faiss_results, usearch_results\n","    \n","    \n","    def get_image_paths(self, indices: List[int]) -> List[str]:\n","        return [self.global_index2image_path[i] for i in indices]\n","\n","    def load_indexes(self, faiss_path: str, usearch_path: str, global2imgpath_path: str):\n","        self.load_faiss_index(faiss_path)\n","        self.load_usearch_index(usearch_path)\n","        with open(global2imgpath_path, 'r') as f:\n","            self.global_index2image_path = json.load(f)\n","        print(\"All indexes and mappings loaded successfully.\")\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:18:46.281213Z","iopub.status.busy":"2024-07-31T08:18:46.280947Z","iopub.status.idle":"2024-07-31T08:18:46.295400Z","shell.execute_reply":"2024-07-31T08:18:46.294601Z","shell.execute_reply.started":"2024-07-31T08:18:46.281190Z"},"trusted":true},"outputs":[],"source":["class Translation:\n","    def __init__(self, from_lang='vi', to_lang='en', mode='google'):\n","        self.__mode = mode\n","        self.__from_lang = from_lang\n","        self.__to_lang = to_lang\n","\n","        if mode == 'googletrans':\n","            self.translator = googletrans.Translator()\n","        elif mode == 'translate':\n","            self.translator = translate.Translator(from_lang=from_lang, to_lang=to_lang)\n","\n","    def preprocessing(self, text):\n","        return text.lower()\n","\n","    def __call__(self, text):\n","        text = self.preprocessing(text)\n","        return self.translator.translate(text) if self.__mode == 'translate' \\\n","                else self.translator.translate(text, dest=self.__to_lang).text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:18:46.296586Z","iopub.status.busy":"2024-07-31T08:18:46.296326Z","iopub.status.idle":"2024-07-31T08:18:46.306699Z","shell.execute_reply":"2024-07-31T08:18:46.305981Z","shell.execute_reply.started":"2024-07-31T08:18:46.296564Z"},"trusted":true},"outputs":[],"source":["def display_result(indices, embedder, k=20):\n","    k = min(k, len(indices))\n","    \n","    n_cols = 3 \n","    n_rows = (k + n_cols - 1) // n_cols\n","    \n","    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 7 * n_rows)) \n","    \n","    if n_rows == 1:\n","        axes = [axes]\n","    if n_cols == 1:\n","        axes = [[ax] for ax in axes]\n","    \n","    for i, idx in enumerate(indices[:k]):\n","        if i >= k:\n","            break\n","        \n","        row = i // n_cols\n","        col = i % n_cols\n","        \n","        img_path = embedder.get_image_paths([idx])[0]\n","        img = Image.open(img_path)\n","        axes[row][col].imshow(img)\n","        axes[row][col].set_title(f\"Rank: {i+1}\", fontsize=12)\n","        filename = os.path.basename(img_path)\n","        axes[row][col].set_xlabel(filename, fontsize=10, wrap=True)\n","        \n","        axes[row][col].axis('off')\n","        \n","    for i in range(k, n_rows * n_cols):\n","        row = i // n_cols\n","        col = i % n_cols\n","        axes[row][col].axis('off')\n","    \n","    plt.tight_layout()\n","    plt.subplots_adjust(hspace=0.3, wspace=0.1)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:18:46.308270Z","iopub.status.busy":"2024-07-31T08:18:46.307849Z","iopub.status.idle":"2024-07-31T08:18:46.320675Z","shell.execute_reply":"2024-07-31T08:18:46.319836Z","shell.execute_reply.started":"2024-07-31T08:18:46.308223Z"},"trusted":true},"outputs":[],"source":["def get_sorted_query_files(root_dir: str) -> List[str]:\n","    list_querys = []\n","    for dir_root, _, filenames in os.walk(root_dir):\n","        for filename in filenames:\n","            if filename.lower().endswith('.txt'):\n","                list_querys.append(os.path.join(dir_root, filename))\n","    \n","    def sort_key(filepath):\n","        filename = os.path.basename(filepath)\n","        match = re.search(r'p(\\d+)\\.txt$', filename)\n","        if match:\n","            return int(match.group(1))\n","        return 0 \n","    return sorted(list_querys, key=sort_key)\n","\n","def read_queries(file_paths: List[str]) -> List[str]:\n","    queries = []\n","    for file_path in file_paths:\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            queries.append(file.read().strip())\n","    return queries\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T08:33:27.249698Z","iopub.status.busy":"2024-07-31T08:33:27.248514Z","iopub.status.idle":"2024-07-31T08:33:27.255426Z","shell.execute_reply":"2024-07-31T08:33:27.254292Z","shell.execute_reply.started":"2024-07-31T08:33:27.249644Z"},"trusted":true},"outputs":[],"source":["root_directory_query = '/kaggle/input/query-test'\n","image_root_directory = \"/kaggle/input/aic-video\"\n","output_dir = '/kaggle/working/DFN5B_CLIP_ViT_H_14_378'\n","model_name = 'hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n","batch_size = 16"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attempting to load model on cpu\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/fastapi_env/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing images for model: hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\n"]},{"ename":"OSError","evalue":"[Errno 30] Read-only file system: '/kaggle'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embedder \u001b[38;5;241m=\u001b[39m CLIPEmbedding(model_name\u001b[38;5;241m=\u001b[39mmodel_name, model_nick_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDFN5B_CLIP_ViT_H_14_378\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing images for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_image_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_root_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embedder\u001b[38;5;241m.\u001b[39mglobal_index2image_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mCLIPEmbedding.process_image_folder\u001b[0;34m(self, root_dir, output_dir, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image_folder\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m     root_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     33\u001b[0m     output_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     34\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     35\u001b[0m ):\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(root_dir):\n","File \u001b[0;32m/opt/anaconda3/envs/fastapi_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/fastapi_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/fastapi_env/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle'"]}],"source":["embedder = CLIPEmbedding(model_name=model_name, model_nick_name = 'DFN5B_CLIP_ViT_H_14_378')\n","print(f\"\\nProcessing images for model: {model_name}\")\n","embeddings = embedder.process_image_folder(image_root_directory, output_dir, batch_size)\n","if embeddings is not None:\n","    print(f\"Processed {len(embedder.global_index2image_path)} images\")\n","    print(f\"Embedding shape: {embeddings.shape}\")\n","else:\n","    print(f\"Processing images failed for model {model_name}. Please check the image folder path and content.\")\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:32:48.593169Z","iopub.status.idle":"2024-07-31T08:32:48.593584Z","shell.execute_reply":"2024-07-31T08:32:48.593416Z","shell.execute_reply.started":"2024-07-31T08:32:48.593399Z"},"trusted":true},"outputs":[],"source":["translator = Translation(from_lang='vi', to_lang='en', mode='translate')\n","sorted_query_files = get_sorted_query_files(root_directory)\n","queries = read_queries(sorted_query_files)\n","translated_queries = [translator(query) for query in tqdm(queries, desc=\"Translating queries\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:32:48.597001Z","iopub.status.idle":"2024-07-31T08:32:48.597509Z","shell.execute_reply":"2024-07-31T08:32:48.597270Z","shell.execute_reply.started":"2024-07-31T08:32:48.597231Z"},"trusted":true},"outputs":[],"source":["print(\"\\nPerforming text queries\")\n","for query, translated_query in tqdm(zip(queries, translated_queries), total=len(queries), desc=\"Processing queries\"):\n","    print(f\"\\nOriginal Query: {query}\")\n","    print(f\"Translated Query: {translated_query}\")\n","\n","    faiss_results, usearch_results = embedder.text_query(translated_query, k=5)\n","\n","    print(\"FAISS Results:\")\n","    for idx, score in faiss_results:\n","        print(f\"Image: {embedder.get_image_paths([idx])[0]}, Score: {score}\")\n","\n","    print(\"\\nFAISS Results (Visual):\")\n","    faiss_indices = [idx for idx, _ in faiss_results]\n","    display_result(faiss_indices, embedder, k=20)\n","\n","    print(\"\\nUSearch Results:\")\n","    for idx, score in usearch_results:\n","        print(f\"Image: {embedder.get_image_paths([idx])[0]}, Score: {score}\")\n","\n","    print(\"\\nUSearch Results (Visual):\")\n","    usearch_indices = [idx for idx, _ in usearch_results]\n","    display_result(usearch_indices, embedder, k=20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5469694,"sourceId":9068513,"sourceType":"datasetVersion"},{"datasetId":5472772,"sourceId":9072818,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
